# Go AI Assistant

A production-ready AI assistant backend built with Go, featuring modular tool architecture, Redis caching, OpenTelemetry observability, and comprehensive monitoring. Supports OpenAI API with intelligent caching, rate limiting, and circuit breaker patterns for external services.

## Key Features

- ðŸ¤– **AI Integration** - OpenAI API support with configurable models (GPT-4, GPT-4o-mini)
- ðŸ”§ **Modular Tools** - Weather API, calendar, date/time, holidays with plugin architecture
- ðŸ’¾ **Redis Caching** - SHA256-hashed keys for security, configurable TTL, cache hit/miss tracking
- ðŸ“Š **Observability** - OpenTelemetry tracing, Prometheus metrics, structured logging + Real-time cost tracking per user
- ðŸ”’ **Security** - API key authentication, per-IP rate limiting, constant-time comparison
- ðŸ›¡ï¸ **Resilience** - Circuit breaker for external APIs, graceful degradation, retry with exponential backoff
- ðŸ—„ï¸ **MongoDB Storage** - Conversation history with optimized indexes, prompt management
- ðŸ“ **Prompt Management** - Platform-specific prompts with caching and fallback system
- ðŸ”„ **Session Management** - Seamless conversation continuity for stateless clients (Telegram, Web, Mobile)
- ðŸ§  **Intelligent Context Management** - Advanced token counting with tiktoken, AI-powered summarization, hybrid storage strategies
- âœ… **Testing** - Unit, integration, E2E, and performance tests (75%+ coverage)
- ðŸš€ **Production Ready** - Health checks, migrations, backups, CI/CD pipeline

## Architecture

This project follows a clean, production-ready architecture with clear separation of concerns and multiple layers of resilience.

**ðŸ“– For detailed architecture documentation, see [ARCHITECTURE.md](ARCHITECTURE.md)**

### Key Architectural Components

- ðŸ”§ **Modular Tools System** - Extensible plugin architecture for AI capabilities
- ðŸ”„ **Session Management** - Seamless conversation continuity for stateless clients (Telegram, Web, Mobile)
- ðŸ“Š **Observability Stack** - OpenTelemetry tracing + Prometheus metrics + structured logging
- ðŸ›¡ï¸ **Resilience Patterns** - Circuit breakers, retry logic with exponential backoff, Redis caching
- ðŸ” **Security First** - API key auth, rate limiting, constant-time comparison
- ðŸ—„ï¸ **Data Layer** - MongoDB for conversations and prompts, Redis for caching and sessions
- ðŸ“ **Prompt Management** - Dynamic prompt system with platform-specific configurations

### Why This Architecture?

Each architectural decision is **intentional and justified**:

- **Redis Caching**: 10x cost reduction on API calls, 95% faster responses
- **Circuit Breaker**: Prevents cascading failures when external APIs fail
- **Retry Mechanism**: 95%+ success rate on transient errors (invisible to users)
- **Session Management**: Enables stateless clients (bots, web) to maintain context
- **Tool Registry**: Add new AI capabilities without modifying core code
- **Prompt Management**: Dynamic prompts for different platforms and user segments

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed explanations, diagrams, and trade-offs.

---

## Tech Stack

- **Backend**: Go 1.21+, Twirp RPC
- **Database**: MongoDB 7
- **Cache**: Redis 7
- **Observability**: OpenTelemetry, Prometheus
- **Infrastructure**: Docker, Docker Compose
- **CI/CD**: GitHub Actions

## Prerequisites

- Docker & Docker Compose
- Go 1.21 or higher
- OpenAI API key

## Setup

- **Clone and configure**

```bash
git clone https://github.com/YOUR_USERNAME/Go_AI_Assistant.git
cd Go_AI_Assistant
cp .env.example .env
# Edit .env with your API keys
```

## Quick Start

```bash
make up && make migrate-up && make run
# That's all! Api-service is ready, but If you need telegram-bot =) ->

cd python_telegram_bot/ && cp .env.example .env
# Edit .env with your TELEGRAM_BOT_TOKEN
source venv/bin/activate
python telegram_bot_enhanced.py
```

## Ordinary start

1. **Start services**

   ```bash
   docker-compose up -d
   make migrate-up  # Create database indexes and initialize prompts
   ```

2. **Run application**

   ```bash
   go run ./cmd/server
   ```

3. **Verify deployment**

   ```bash
   curl http://localhost:8080/health
   curl http://localhost:8080/metrics
   ```

### API Endpoints

- `GET /health` - Health check (MongoDB + Redis status)
- `GET /ready` - Readiness probe
- `GET /metrics` - Prometheus metrics (requires API key)
- `POST /twirp/chat.ChatService/*` - Chat API (Twirp RPC)

### Interactive API Documentation

The project includes **Swagger UI** for interactive API exploration:

- **Swagger UI**: <http://localhost:8080/docs/>
- **Static Documentation**: <http://localhost:8080/api-docs>

Swagger UI provides:

- Interactive API testing
- Request/response examples
- Automatic schema validation
- All endpoints with detailed descriptions

## Advanced Context Management

The system features sophisticated context management with multiple strategies for optimal performance and resilience:

### Token Counting & Estimation

- **Accurate Token Counting**: Uses `tiktoken-go` library for precise token counting per OpenAI model
- **Fallback Estimation**: Graceful fallback to character-based estimation when tiktoken fails
- **Model-specific Encoding**: Automatic encoding selection based on OpenAI model (cl100k_base for GPT-4/3.5)

### Context Summarization Strategies

- **AI-Powered Summarization**: Uses GPT-4 to create intelligent summaries of long conversations
- **Basic Reduction**: Fallback strategy that keeps only recent messages
- **Hybrid Approach**: Tries AI summarization first, falls back to basic reduction if needed

### Storage Strategies

- **Redis Storage**: Primary persistent storage for conversation context with automatic cleanup
- **Memory Storage**: In-memory fallback for high availability (removed in favor of unified approach)
- **Unified Context Manager**: Single ContextManager with Redis storage and AI summarization capabilities

### Intelligent Context Reduction & Summarization

- **Proactive Reduction**: Monitors token usage and reduces context before hitting limits
- **Emergency Reduction**: Automatic context reduction when OpenAI returns context length errors
- **AI-Powered Summarization**: Uses GPT-4 to create intelligent summaries of long conversations
- **Automatic Retry**: After summarization, the request is automatically retried with reduced context
- **Configurable Limits**: Model-specific token limits with safety margins (80-90% of model capacity)

### Context Management Strategies

- **AI Summarization Strategy**: Uses GPT-4 to create concise summaries while preserving key information
- **Basic Reduction Strategy**: Fallback that keeps only recent messages when AI summarization fails
- **Hybrid Approach**: Tries AI summarization first, falls back to basic reduction if needed
- **Redis Storage**: Persistent context storage with automatic cleanup

### Fault Tolerance Features

- **Automatic Recovery**: System doesn't crash when context limits are exceeded
- **Retry Mechanism**: Failed requests due to context limits are automatically retried after summarization
- **Graceful Degradation**: Falls back to basic reduction when AI summarization is unavailable
- **Error Detection**: Automatically detects context length errors from OpenAI API

### Configuration

```bash
# Context Management Configuration
MAX_CONTEXT_TOKENS=4000                  # Maximum context tokens per conversation
MAX_CONTEXT_HISTORY=50                   # Maximum message history to keep
```

## Available Tools

The assistant comes with a scalable modular tool system.

## Testing

```bash
# Run all tests
make test

# Specific test suites
make test-unit          # Unit tests
make test-integration   # Integration tests (requires Docker)
make test-e2e          # End-to-end tests
make test-performance  # Benchmarks

# Coverage report
make test-coverage

# Smoke test (full system validation)
make smoke
```

## Configuration

Key environment variables (see `.env.example`):

```bash
# Required
OPENAI_API_KEY=sk-...                    # OpenAI API key
MONGO_URI=mongodb://...                   # MongoDB connection string
REDIS_ADDR=localhost:6379                 # Redis address

# Optional - AI Configuration
OPENAI_MODEL=gpt-4o-mini                 # AI model selection
WEATHER_API_KEY=...                       # Weather API key
HOLIDAY_CALENDAR_LINK=...                 # Holiday calendar ICS URL

# API Security & Rate Limiting
API_KEY=changeme_in_production           # API key for /metrics endpoint
API_RATE_LIMIT_RPS=10.0                  # Rate limit (requests/second)
API_RATE_LIMIT_BURST=20                  # Rate limit burst size

# Cache Configuration
CACHE_TTL_HOURS=24                       # Redis cache TTL (hours)
SESSION_TTL_MINUTES=30                   # Session TTL (minutes)

# Circuit Breaker
CIRCUIT_BREAKER_MAX_FAILURES=3           # Max failures before opening
CIRCUIT_BREAKER_COOLDOWN_SECONDS=30      # Cooldown period (seconds)

# Retry Configuration
RETRY_MAX_ATTEMPTS=3                     # Max retry attempts
RETRY_BASE_DELAY_MS=500                  # Base delay (milliseconds)
RETRY_MAX_DELAY_MS=5000                  # Max delay (milliseconds)
```

**Note**: Set `API_KEY` in production to protect the `/metrics` endpoint. Without it, metrics are publicly accessible.

## Monitoring

**Prometheus Metrics** (`/metrics`):

### HTTP Metrics

- `http_requests_total` - Total HTTP requests (by method, path, status)
- `http_request_duration_seconds` - Request latency histogram
- `http_requests_in_flight` - Active requests counter

### OpenAI Metrics (Token Usage & Cost Tracking)

- `openai_tokens_input_total` - Total input tokens consumed (by operation, model, user_id, platform)
- `openai_tokens_output_total` - Total output tokens consumed (by operation, model, user_id, platform)
- `openai_tokens_total` - Total tokens consumed (by operation, model, user_id, platform)
- `openai_requests_total` - Total OpenAI API requests (by operation, model, user_id, platform)
- `openai_request_duration_ms` - OpenAI API request duration histogram

### Circuit Breaker Metrics

- `weather_circuit_state` - Circuit breaker status (open/closed/half-open)

### Example Prometheus Queries

**Track token usage per user:**

```promql
# Tokens consumed by user in last hour
sum by (user_id) (
  rate(openai_tokens_total{user_id!=""}[1h])
) * 3600
```

**Calculate costs (GPT-4: $0.03/1K input, $0.06/1K output):**

```promql
# Total cost estimate
sum(openai_tokens_input_total) / 1000 * 0.03 +
sum(openai_tokens_output_total) / 1000 * 0.06
```

**Find top 10 most active users:**

```promql
topk(10, 
  sum by (user_id) (openai_tokens_total{user_id!=""})
)
```

**Health Checks**:

```bash
curl http://localhost:8080/health | jq
# {
#   "status": "healthy",
#   "checks": {
#     "mongodb": "ok",
#     "redis": "ok"
#   }
# }
```

**Logs**: Structured JSON with trace IDs for request correlation

## Database Management

```bash
# Migrations
make migrate-up      # Apply migrations and initialize prompts
make migrate-down    # Rollback migrations
make migrate-status  # Check current state

# Backups
make backup                              # Create timestamped backup
make restore BACKUP_PATH=backups/...    # Restore from backup
```

## ðŸ¤– Telegram Bot Integration

The project includes a **production-ready Telegram bot** that demonstrates the session-based conversation management capabilities of the system.

### Features

- âœ… **Automatic Conversation Continuity** - Users can have ongoing conversations without managing conversation IDs
- âœ… **Session Recovery** - Sessions survive Redis restarts (MongoDB fallback)
- âœ… **Real-time Weather Queries** - Direct integration with weather tools
- âœ… **Full AI Assistant Capabilities** - All OpenAI features available via Telegram
- âœ… **Platform-specific Prompts** - Customized responses for Telegram users

### Quick Start

```bash
# Navigate to bot directory
cd python_telegram_bot

# Create virt env
python -m venv venv

# Install dependencies
pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env and set TELEGRAM_BOT_TOKEN

# Run bot
python telegram_bot_enhanced.py
```

### How It Works

The Telegram bot uses the `session_metadata` pattern to maintain conversation context:

```python
# Bot sends request to Go backend
{
  "message": "What's the weather in Barcelona?",
  "session_metadata": {
    "platform": "telegram",
    "user_id": "12345",
    "chat_id": "67890"
  }
}
```

The backend automatically:

1. Creates or retrieves a session from Redis
2. Maps it to a conversation in MongoDB
3. Maintains context across messages
4. Handles session expiration and recovery
5. Uses platform-specific prompts for better user experience

**Note:** Session management works for **any stateless client** (web frontends, mobile apps, chatbots). Telegram is just one example implementation.

See [python_telegram_bot/README.md](python_telegram_bot/README.md) for detailed setup and usage.

---

## API Usage Examples

### Using curl

**Start a new conversation:**

```bash
curl -X POST http://localhost:8080/twirp/acai.chat.ChatService/StartConversation \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What is the weather in Barcelona?"
  }'
```

**Response:**

```json
{
  "conversation_id": "507f1f77bcf86cd799439011",
  "title": "Weather in Barcelona",
  "reply": "The weather in Barcelona is currently sunny with a temperature of 22Â°C..."
}
```

**Continue conversation (with conversation_id):**

```bash
curl -X POST http://localhost:8080/twirp/acai.chat.ChatService/ContinueConversation \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "507f1f77bcf86cd799439011",
    "message": "What about tomorrow?"
  }'
```

**Continue conversation (with session_metadata - stateless):**

```bash
curl -X POST http://localhost:8080/twirp/acai.chat.ChatService/ContinueConversation \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What about tomorrow?",
    "session_metadata": {
      "platform": "web",
      "user_id": "user123",
      "chat_id": "session456"
    }
  }'
```

**List conversations:**

```bash
curl -X POST http://localhost:8080/twirp/acai.chat.ChatService/ListConversations \
  -H "Content-Type: application/json" \
  -d '{}'
```

**Get conversation details:**

```bash
curl -X POST http://localhost:8080/twirp/acai.chat.ChatService/DescribeConversation \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "507f1f77bcf86cd799439011"
  }'
```

### Using the CLI Tool

```bash
# Start interactive conversation
go run ./cmd/cli ask

# Continue existing conversation
go run ./cmd/cli ask <conversation-id>

# List all conversations
go run ./cmd/cli list

# Show conversation details
go run ./cmd/cli show <conversation-id>
```

See [cmd/cli/README.md](cmd/cli/README.md) for more CLI examples.

---

## Prompt Management System

The assistant features a sophisticated prompt management system:

### Features

- **Platform-specific prompts**: Different prompts for web, telegram, mobile
- **User segment targeting**: Custom prompts for different user groups
- **Caching**: Redis caching for performance
- **Fallback system**: Default prompts when custom ones aren't available
- **Dynamic updates**: Update prompts without restarting the application

### Prompt Types

- **System Prompt**: Main assistant behavior and personality
- **Title Generation**: How conversation titles are generated
- **Platform-specific**: Custom prompts for different interfaces

### Configuration

Prompts are stored in MongoDB and can be managed through the database. The system automatically loads and caches prompts based on platform and user segment.

## Project Structure

```
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ server/          # Main application
â”‚   â””â”€â”€ cli/             # CLI tools
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ chat/            # Chat service implementation
â”‚   â”œâ”€â”€ chat/assistant/  # AI assistant with tool orchestration
â”‚   â”œâ”€â”€ chat/model/      # Domain models and repository
â”‚   â”œâ”€â”€ circuitbreaker/  # Circuit breaker pattern
â”‚   â”œâ”€â”€ config/          # Configuration management
â”‚   â”œâ”€â”€ errorsx/         # Error handling utilities
â”‚   â”œâ”€â”€ health/          # Health check endpoints
â”‚   â”œâ”€â”€ httpx/           # HTTP middleware (auth, rate limit, logging)
â”‚   â”œâ”€â”€ metrics/         # Prometheus metrics
â”‚   â”œâ”€â”€ otel/            # OpenTelemetry configuration
â”‚   â”œâ”€â”€ redisx/          # Redis cache layer
â”‚   â”œâ”€â”€ retry/           # Retry mechanism with exponential backoff
â”‚   â”œâ”€â”€ session/         # Session management
â”‚   â”œâ”€â”€ tools/           # Modular tool system
â”‚   â”‚   â”œâ”€â”€ datetime/    # Date/time tool
â”‚   â”‚   â”œâ”€â”€ factory/     # Tool factory
â”‚   â”‚   â”œâ”€â”€ holidays/    # Holiday calendar tool
â”‚   â”‚   â”œâ”€â”€ registry/    # Tool registry
â”‚   â”‚   â””â”€â”€ weather/     # Weather tool
â”‚   â””â”€â”€ weather/         # Weather service with caching
â”œâ”€â”€ python_telegram_bot/ # Telegram bot integration
â”œâ”€â”€ migrations/          # Database migrations
â”œâ”€â”€ scripts/             # Utility scripts
â””â”€â”€ tests/              # Test suites
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

**Code Quality**: All PRs must pass CI checks (tests, linting, formatting)

## License

MIT License (see LICENSE file)

## Links

- [Architecture Documentation](ARCHITECTURE.md) - Detailed system design and rationale
- [Production Readiness Checklist](PRODUCTION_READINESS.md) - Deployment guidelines
- [Interactive API Documentation](http://localhost:8080/docs/) - Swagger UI for API testing
- [Static API Documentation](http://localhost:8080/api-docs) - HTML documentation
- [Telegram Bot Setup](python_telegram_bot/README.md) - Bot integration guide
- [CLI Tool Usage](cmd/cli/README.md) - Command-line interface
